<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Your Title - ai</title>
 <link href="http://example.com/tag/ai.xml" rel="self"/>
 <link href="http://example.com/tag/ai.html"/>
 <updated>2024-01-19T15:32:50-08:00</updated>
 <id>http://example.com/tag/ai.html</id>
 <author>
   <name>Author Here</name>
 </author>
 
 <entry>
   <title>Accountability with AI</title>
   <link href="http://example.com/2023/06/20/ai-accountability.html"/>
   <updated>2023-06-20T00:00:00-07:00</updated>
   <id>http://example.com/2023/06/20/ai-accountability</id>
   <content type="html">&lt;p&gt;This post is a public version of my comments submitted to the &lt;a href=&quot;https://www.regulations.gov/document/OSTP-TECH-2023-0007-0001&quot;&gt;US government’s request for information on artificial intelligence&lt;/a&gt;. As such, it’s primarily plain text and contains little formatting, as it was submitted as plain text. The examples are simplified to be as comprehensive to the layman as possible.&lt;/p&gt;

&lt;p&gt;For some reason, the government makes the request for comments inordinately hard to find. Tarah Wheeler has an excellent post on how hard it is to find &lt;a href=&quot;https://www.tarah.org/2023/06/10/yes-but-how-do-i-submit-a-comment-on-how-unintuitive-the-comment-submission-process-is/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;comments-to-the-office-of-science-and-technology-policy&quot;&gt;Comments to the Office of Science and Technology Policy&lt;/h2&gt;
&lt;p&gt;My comment is specifically about safety measures that must be taken in the development of AI technology to ensure that people’s rights and safety are protected in an equitable manner.&lt;/p&gt;

&lt;p&gt;If no other part of this comment is read, it’s imperative that this one thing be communicated: &lt;strong&gt;Computers cannot be held accountable, and therefore must not make decisions unsupervised.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Due to complicated issues involving the nature of problem-solving computer programs, computers may come to conclusions that seem reasonable to them but are unacceptable or dangerous to people. To prevent them from automatically acting upon these conclusions, it’s imperative that a person review and approve suggested actions provided by computer assistance before they are enacted.&lt;/p&gt;

&lt;p&gt;Consider the example of a computer system assigning dorm rooms at a college. Imagine that it evaluates how well matched people are to their preferred rooms using a score, and will select the highest scoring arrangement. If one person is in a room that they prefer, the arrangement gets 1 point. We wouldn’t want someone in a wheelchair to be in an upstairs room that they couldn’t get to, so we might assign this situation -1000 points. Unfortunately, this would let the computer select a situation where 1002 people get their preferred rooms and 1 person in a wheelchair gets an inaccessible room, over a situation where 1 person gets the room they want, and everyone else gets an acceptable but not preferred room. If the computer had no oversight, it might deem this an acceptable solution, and nobody would realize until a disabled person tried to move into their second floor room on the first day of school.&lt;/p&gt;

&lt;p&gt;Additionally, there remains the possibility of purposeful accountability dodging. &lt;strong&gt;Humans must be responsible for the decisions of computers they create and run.&lt;/strong&gt; Otherwise, unscrupulous parties could outsource any unethical decision to a computer, and completely avoid liability for any damage caused. For this reason, it’s of paramount importance that usage of AI tools not diminish one’s culpability (legal or otherwise) in making poor decisions.&lt;/p&gt;

</content>
 </entry>
 
</feed>